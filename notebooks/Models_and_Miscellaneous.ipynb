{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../src')\n",
    "from src import utils\n",
    "from src import generators\n",
    "import imp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for data in testloader:\n",
    "    print(data[0].shape, torch.randn(1).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Mode - CNN_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(VGG,self).__init__()\n",
    "        pad = 1\n",
    "        \n",
    "        self.cnn = nn.Sequential(nn.BatchNorm2d(1),\n",
    "                                     nn.Conv2d(1,32,3,padding=pad),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.BatchNorm2d(32),\n",
    "                                     nn.Conv2d(32,32,3,padding=pad),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.MaxPool2d(2,2), \n",
    "        \n",
    "                                     nn.BatchNorm2d(32),\n",
    "                                     nn.Conv2d(32,64,3,padding=pad),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.BatchNorm2d(64),\n",
    "                                     nn.Conv2d(64,64,3,padding=pad),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.MaxPool2d(2,2),\n",
    "                                     \n",
    "                                     nn.BatchNorm2d(64),\n",
    "                                     nn.Conv2d(64,128,3,padding=pad),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.BatchNorm2d(128),\n",
    "                                     nn.Conv2d(128,128,3,padding=pad),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.MaxPool2d(2,2),\n",
    "        \n",
    "                                     nn.BatchNorm2d(128),\n",
    "                                     nn.Conv2d(128,256,3,padding=pad),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.BatchNorm2d(256),\n",
    "                                     nn.Conv2d(256,256,3,padding=pad),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.MaxPool2d(2,2), \n",
    "        \n",
    "                                     nn.BatchNorm2d(256),\n",
    "                                     nn.Conv2d(256,256,3,padding=pad),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.BatchNorm2d(256),\n",
    "                                     nn.Conv2d(256,256,3,padding=pad),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.MaxPool2d(2,2),\n",
    "                                     \n",
    "                                     nn.BatchNorm2d(256),\n",
    "                                     nn.Conv2d(256,512,3,padding=pad),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.BatchNorm2d(512),\n",
    "                                     nn.Conv2d(512,512,3,padding=pad),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.MaxPool2d(2,2))\n",
    "  \n",
    "        self.fc1 = nn.Sequential(nn.Linear(8192, 1096), \n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Dropout(0.8),\n",
    "                                     nn.Linear(1096, 96),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Dropout(0.9),\n",
    "                                     nn.Linear(96, 1))\n",
    "\n",
    "        # self.fc2 = nn.Sequential(nn.Linear(8192, 4096), \n",
    "        #                              nn.ReLU(),\n",
    "        #                              nn.Dropout(0.8),\n",
    "        #                              nn.Linear(4096, 4096),\n",
    "        #                              nn.ReLU(),\n",
    "        #                              nn.Dropout(0.9),\n",
    "        #                              nn.Linear(4096, 1))\n",
    "        \n",
    "    def forward(self, x, batch_size=1, mvcnn=False):\n",
    "        \n",
    "        if mvcnn:\n",
    "            view_pool = []\n",
    "            # Assuming x has shape (x, 1, 299, 299)\n",
    "            for n, v in enumerate(x):\n",
    "                v = v.unsqueeze(0)\n",
    "                v = self.cnn(v)\n",
    "                v = v.view(v.size(0), 512 * 4 * 4)\n",
    "                view_pool.append(v)\n",
    "\n",
    "            pooled_view = view_pool[0]\n",
    "            for i in range(1, len(view_pool)):\n",
    "                pooled_view = torch.max(pooled_view, view_pool[i])\n",
    "\n",
    "            output = self.fc1(pooled_view)\n",
    "        \n",
    "        else:\n",
    "            x = self.cnn(x)\n",
    "            x = x.view(-1, 512 * 4* 4)\n",
    "            x = self.fc1(x)\n",
    "            output = F.sigmoid(x)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm2d-1          [-1, 1, 299, 299]               2\n",
      "            Conv2d-2         [-1, 32, 299, 299]             320\n",
      "              ReLU-3         [-1, 32, 299, 299]               0\n",
      "       BatchNorm2d-4         [-1, 32, 299, 299]              64\n",
      "            Conv2d-5         [-1, 32, 299, 299]           9,248\n",
      "              ReLU-6         [-1, 32, 299, 299]               0\n",
      "         MaxPool2d-7         [-1, 32, 149, 149]               0\n",
      "       BatchNorm2d-8         [-1, 32, 149, 149]              64\n",
      "            Conv2d-9         [-1, 64, 149, 149]          18,496\n",
      "             ReLU-10         [-1, 64, 149, 149]               0\n",
      "      BatchNorm2d-11         [-1, 64, 149, 149]             128\n",
      "           Conv2d-12         [-1, 64, 149, 149]          36,928\n",
      "             ReLU-13         [-1, 64, 149, 149]               0\n",
      "        MaxPool2d-14           [-1, 64, 74, 74]               0\n",
      "      BatchNorm2d-15           [-1, 64, 74, 74]             128\n",
      "           Conv2d-16          [-1, 128, 74, 74]          73,856\n",
      "             ReLU-17          [-1, 128, 74, 74]               0\n",
      "      BatchNorm2d-18          [-1, 128, 74, 74]             256\n",
      "           Conv2d-19          [-1, 128, 74, 74]         147,584\n",
      "             ReLU-20          [-1, 128, 74, 74]               0\n",
      "        MaxPool2d-21          [-1, 128, 37, 37]               0\n",
      "      BatchNorm2d-22          [-1, 128, 37, 37]             256\n",
      "           Conv2d-23          [-1, 256, 37, 37]         295,168\n",
      "             ReLU-24          [-1, 256, 37, 37]               0\n",
      "      BatchNorm2d-25          [-1, 256, 37, 37]             512\n",
      "           Conv2d-26          [-1, 256, 37, 37]         590,080\n",
      "             ReLU-27          [-1, 256, 37, 37]               0\n",
      "        MaxPool2d-28          [-1, 256, 18, 18]               0\n",
      "      BatchNorm2d-29          [-1, 256, 18, 18]             512\n",
      "           Conv2d-30          [-1, 256, 18, 18]         590,080\n",
      "             ReLU-31          [-1, 256, 18, 18]               0\n",
      "      BatchNorm2d-32          [-1, 256, 18, 18]             512\n",
      "           Conv2d-33          [-1, 256, 18, 18]         590,080\n",
      "             ReLU-34          [-1, 256, 18, 18]               0\n",
      "        MaxPool2d-35            [-1, 256, 9, 9]               0\n",
      "      BatchNorm2d-36            [-1, 256, 9, 9]             512\n",
      "           Conv2d-37            [-1, 512, 9, 9]       1,180,160\n",
      "             ReLU-38            [-1, 512, 9, 9]               0\n",
      "      BatchNorm2d-39            [-1, 512, 9, 9]           1,024\n",
      "           Conv2d-40            [-1, 512, 9, 9]       2,359,808\n",
      "             ReLU-41            [-1, 512, 9, 9]               0\n",
      "        MaxPool2d-42            [-1, 512, 4, 4]               0\n",
      "           Linear-43                 [-1, 1096]       8,979,528\n",
      "             ReLU-44                 [-1, 1096]               0\n",
      "          Dropout-45                 [-1, 1096]               0\n",
      "           Linear-46                   [-1, 96]         105,312\n",
      "             ReLU-47                   [-1, 96]               0\n",
      "          Dropout-48                   [-1, 96]               0\n",
      "           Linear-49                    [-1, 1]              97\n",
      "================================================================\n",
      "Total params: 14,980,715\n",
      "Trainable params: 14,980,715\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.34\n",
      "Forward/backward pass size (MB): 229.40\n",
      "Params size (MB): 57.15\n",
      "Estimated Total Size (MB): 286.89\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allwyn/venv36/lib/python3.6/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "model = VGG().to(device)\n",
    "summary(model, (1, 299, 299))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since patients have varying images, create single images where the channels occupy the slices of the patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new() received an invalid combination of arguments - got (NoneType, int), but expected one of:\n * (torch.device device)\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, torch.device device)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mNoneType\u001b[0m, \u001b[31;1mint\u001b[0m)\n * (object data, torch.device device)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mNoneType\u001b[0m, \u001b[31;1mint\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-92239081e971>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmvcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMVCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-52dbad35b7d3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes)\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                                      \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                                      nn.Linear(4096, num_classes))\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmvcnn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv36/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (NoneType, int), but expected one of:\n * (torch.device device)\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, torch.device device)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mNoneType\u001b[0m, \u001b[31;1mint\u001b[0m)\n * (object data, torch.device device)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mNoneType\u001b[0m, \u001b[31;1mint\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mvcnn = MVCNN().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(mvcnn.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/alex/Dataset 1/Dataset - 1.xlsx'\n",
    "df = pd.read_excel(file_path, sheet_name='Feuil1')\n",
    "\n",
    "edss = df['EDSS'].tolist()\n",
    "p_id = df['Sequence_id'].tolist()\n",
    "channels = 1\n",
    "resize = 299\n",
    "normalization = 'min-max'\n",
    "\n",
    "patient_information = [(p_id[i], edss[i]) for i in range(df.shape[0])]\n",
    "train_patient_information = patient_information[:int(0.9*len(patient_information))]\n",
    "valid_patient_information = patient_information[int(0.9*len(patient_information)):]\n",
    "base_DatabasePath = '/home/alex/Dataset 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_inst = generators.SEPGenerator(base_DatabasePath, \n",
    "                                                channels=channels,\n",
    "                                                resize=resize,\n",
    "                                                normalization=normalization)\n",
    "\n",
    "train_generator = generator_inst.generator(train_patient_information)\n",
    "valid_generator = generator_inst.generator(valid_patient_information)\n",
    "\n",
    "#dataloader = torch.utils.data.DataLoader(train_generator, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On_Going_Epoch : 1 \t | Iteration : 50 \t | Training Loss : 4511.05859375\n",
      "On_Going_Epoch : 1 \t | Iteration : 100 \t | Training Loss : 2339.1357421875\n",
      "tensor(7.8825, device='cuda:0')\n",
      "tensor(7.8826, device='cuda:0')\n",
      "tensor(8.5348, device='cuda:0')\n",
      "tensor(10.2445, device='cuda:0')\n",
      "tensor(18.1271, device='cuda:0')\n",
      "tensor(18.1641, device='cuda:0')\n",
      "tensor(19.9259, device='cuda:0')\n",
      "tensor(20.5781, device='cuda:0')\n",
      "tensor(20.6151, device='cuda:0')\n",
      "tensor(28.4977, device='cuda:0')\n",
      "tensor(203.7557, device='cuda:0')\n",
      "tensor(211.6382, device='cuda:0')\n",
      "tensor(970.9899, device='cuda:0')\n",
      "tensor(985.4875, device='cuda:0')\n",
      "tensor(999.9852, device='cuda:0')\n",
      "tensor(1000.0223, device='cuda:0')\n",
      "tensor(1000.0593, device='cuda:0')\n",
      "tensor(1820.6040, device='cuda:0')\n",
      "tensor(1831.5441, device='cuda:0')\n",
      "tensor(1842.4841, device='cuda:0')\n",
      "tensor(1853.4242, device='cuda:0')\n",
      "tensor(1864.3643, device='cuda:0')\n",
      "tensor(1864.4012, device='cuda:0')\n",
      "tensor(1864.4382, device='cuda:0')\n",
      "tensor(1872.3208, device='cuda:0')\n",
      "tensor(1926.3999, device='cuda:0')\n",
      "tensor(1960.1279, device='cuda:0')\n",
      "tensor(1993.8560, device='cuda:0')\n",
      "tensor(2176.1489, device='cuda:0')\n",
      "tensor(2197.5427, device='cuda:0')\n",
      "tensor(2212.0405, device='cuda:0')\n",
      "tensor(2226.5383, device='cuda:0')\n",
      "tensor(2241.0361, device='cuda:0')\n",
      "tensor(2241.0732, device='cuda:0')\n",
      "tensor(2244.3406, device='cuda:0')\n",
      "tensor(2278.0686, device='cuda:0')\n",
      "tensor(2311.7966, device='cuda:0')\n",
      "tensor(2345.5247, device='cuda:0')\n",
      "tensor(2345.5618, device='cuda:0')\n",
      "tensor(2497.3787, device='cuda:0')\n",
      "tensor(2498.9541, device='cuda:0')\n",
      "tensor(2502.2214, device='cuda:0')\n",
      "tensor(2542.0071, device='cuda:0')\n",
      "tensor(2581.7927, device='cuda:0')\n",
      "tensor(2905.2920, device='cuda:0')\n",
      "tensor(6808.9893, device='cuda:0')\n",
      "tensor(6835.3154, device='cuda:0')\n",
      "tensor(6837.0254, device='cuda:0')\n",
      "Epoch : 1 \t | Training Loss : 2339.1357421875 \t | Validation Loss : 142.4380340576172 \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exception' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/JFRDataChallenge/src/generators.py\u001b[0m in \u001b[0;36m__extract_DCMImage\u001b[0;34m(self, dcm_path)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydicom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdcmread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdcm_path\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# Read dcm file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_array\u001b[0m                                 \u001b[0;31m# Exctact image from dcm files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv36/lib/python3.6/site-packages/pydicom/filereader.py\u001b[0m in \u001b[0;36mdcmread\u001b[0;34m(fp, defer_size, stop_before_pixels, force, specific_tags)\u001b[0m\n\u001b[1;32m    879\u001b[0m         dataset = read_partial(fp, stop_when, defer_size=defer_size,\n\u001b[0;32m--> 880\u001b[0;31m                                force=force, specific_tags=specific_tags)\n\u001b[0m\u001b[1;32m    881\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv36/lib/python3.6/site-packages/pydicom/filereader.py\u001b[0m in \u001b[0;36mread_partial\u001b[0;34m(fileobj, stop_when, defer_size, force, specific_tags)\u001b[0m\n\u001b[1;32m    757\u001b[0m                                \u001b[0mstop_when\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_when\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m                                specific_tags=specific_tags)\n\u001b[0m\u001b[1;32m    759\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv36/lib/python3.6/site-packages/pydicom/filereader.py\u001b[0m in \u001b[0;36mread_dataset\u001b[0;34m(fp, is_implicit_VR, is_little_endian, bytelength, stop_when, defer_size, parent_encoding, specific_tags)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mraw_data_elements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6f8289f3e3b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtotal_TrainLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mt_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_item\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mimage_3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/JFRDataChallenge/src/generators.py\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(self, patient_InfoDatabase, max_slices, dark_matter, shuffle)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mdcm_image\u001b[0m               \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__extract_DCMImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatient_dcm_FilePath\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# extract image from .dcm file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[0mpreproc_image\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreproc_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdcm_image\u001b[0m\u001b[0;34m)\u001b[0m                                                 \u001b[0;31m# preprocess image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mtransform_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreproc_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# transform the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/JFRDataChallenge/src/generators.py\u001b[0m in \u001b[0;36m__extract_DCMImage\u001b[0;34m(self, dcm_path)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exception' is not defined"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "train_iterations = 100\n",
    "valid_iterations = len(valid_patient_information)\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_TrainLoss = 0\n",
    "\n",
    "    for t_m, t_item in enumerate(train_generator):\n",
    "\n",
    "        image_3D, label = torch.tensor(t_item[0], device=device).float(), torch.tensor(t_item[1], device=device).float()\n",
    "        output = mvcnn(image_3D, 1)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_TrainLoss += loss\n",
    "\n",
    "        if not (t_m+1)%50:\n",
    "            print(\"On_Going_Epoch : {} \\t | Iteration : {} \\t | Training Loss : {}\".format(epoch+1, t_m+1, total_TrainLoss/(t_m+1)))\n",
    "\n",
    "        if (t_m+1) == train_iterations:\n",
    "            total_ValidLoss = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for v_m, v_item in enumerate(valid_generator):\n",
    "                    image_3D, label = torch.tensor(v_item[0], device=device).float(), torch.tensor(v_item[1], device=device).float()\n",
    "                    output = mvcnn(image_3D, 1)\n",
    "                    total_ValidLoss += criterion(output, label)\n",
    "                    print(total_ValidLoss)\n",
    "                    if (v_m + 1) == valid_iterations:\n",
    "                        break\n",
    "                    \n",
    "            print(\"Epoch : {} \\t | Training Loss : {} \\t | Validation Loss : {} \".format(epoch+1, total_TrainLoss/(t_m+1), total_ValidLoss/(v_m+1)) )                   \n",
    "\n",
    "            torch.save(mvcnn, './' + 'vgg_' + str(epoch) + '.pkl')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_ValidLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./vgg_9'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "c = torch.randn(90, 512, 4, 4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "#torch.randn(90, 1, 299, 299)\n",
    "for n,v in enumerate(c):\n",
    "    \n",
    "    v = v.view(1, 512*4*4).to(device)\n",
    "    print(n)\n",
    "    if n:\n",
    "        pooled_view = torch.max(pooled_view, v).to(device)\n",
    "    else:\n",
    "        pooled_view = v.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(image, transformation='original', angle=30):\n",
    "    \"\"\"\n",
    "    Function to generate images based on the requested transfomations\n",
    "    Args:\n",
    "    - image             (nd.array)  : input image array\n",
    "    - transformation    (str)       : image transformation to be effectuated\n",
    "    - angle \t\t(int)\t    : rotation angle if transformation is a rotation\n",
    "    Returns:\n",
    "    - trans_image       (nd.array)  : transformed image array\n",
    "    \"\"\"\n",
    "\n",
    "    def rotateImage(image, angle):\n",
    "        \"\"\"\n",
    "        Function to rotate an image at its center\n",
    "        \"\"\"\n",
    "        image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "        rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
    "        result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "        return result\n",
    "    \n",
    "    # Image transformations\n",
    "    if transformation == 'original':\n",
    "        trans_image = image\n",
    "    elif transformation == 'flip_v':\n",
    "        trans_image = cv2.flip(image, 0)\n",
    "    elif transformation == 'flip_h':\n",
    "        trans_image = cv2.flip(image, 1)\n",
    "    elif transformation == 'flip_vh':\n",
    "        trans_image = cv2.flip(image, -1)\n",
    "    elif transformation == 'rot_c':\n",
    "        trans_image = rotateImage(image, -angle)\n",
    "    elif transformation == 'rot_ac':\n",
    "        trans_image = rotateImage(image, angle)\n",
    "    else:\n",
    "        raise ValueError(\"In valid transformation value passed : {}\".format(transformation))\n",
    "\n",
    "    return trans_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The agumenter ought to be able to do the following:\n",
    "- Get list of patient paths and their respective scores (make sure to do the validation and test splits before)\n",
    "    - Select a random augmentation (flag='test')\n",
    "    - Select a patient path and his/her corresponding score\n",
    "    - With each .dcm file do following: \n",
    "        - read image\n",
    "        - normalized image\n",
    "        - resize image\n",
    "        - get percentage of white matter (%, n) and append to list\n",
    "        - transform image\n",
    "        - store in an array\n",
    "    - yield image_3D (top 70 images with white matter), label\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SEP_generator(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 resize,\n",
    "                 normalization,\n",
    "                 transformations)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "class ImageBaseAug(object):\n",
    "    def __init__(self):\n",
    "        sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "        self.seq = iaa.Sequential(\n",
    "            [\n",
    "                # Blur each image with varying strength using\n",
    "                # gaussian blur (sigma between 0 and 3.0),\n",
    "                # average/uniform blur (kernel size between 2x2 and 7x7)\n",
    "                # median blur (kernel size between 3x3 and 11x11).\n",
    "                iaa.OneOf([\n",
    "                    iaa.GaussianBlur((0, 3.0)),\n",
    "                    iaa.AverageBlur(k=(2, 7)),\n",
    "                    iaa.MedianBlur(k=(3, 11)),\n",
    "                ]),\n",
    "                # Sharpen each image, overlay the result with the original\n",
    "                # image using an alpha between 0 (no sharpening) and 1\n",
    "                # (full sharpening effect).\n",
    "                sometimes(iaa.Sharpen(alpha=(0, 0.5), lightness=(0.75, 1.5))),\n",
    "                # Add gaussian noise to some images.\n",
    "                sometimes(iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5)),\n",
    "                # Add a value of -5 to 5 to each pixel.\n",
    "                sometimes(iaa.Add((-5, 5), per_channel=0.5)),\n",
    "                # Change brightness of images (80-120% of original value).\n",
    "                sometimes(iaa.Multiply((0.8, 1.2), per_channel=0.5)),\n",
    "                # Improve or worsen the contrast of images.\n",
    "                sometimes(iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5)),\n",
    "            ],\n",
    "            # do all of the above augmentations in random order\n",
    "            random_order=True\n",
    "        )\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        seq_det = self.seq.to_deterministic()\n",
    "        image, label = sample['image'], sample['label']\n",
    "        image = seq_det.augment_images([image])[0]\n",
    "        return {'image': image, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )   \n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class=1):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(1, 32)\n",
    "        self.dconv_down2 = double_conv(32, 64)\n",
    "        self.dconv_down3 = double_conv(64, 128)\n",
    "        self.dconv_down4 = double_conv(128, 256)       \n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
    "        \n",
    "        self.dconv_up3 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up2 = double_conv(64 + 128, 64)\n",
    "        self.dconv_up1 = double_conv(32 + 64, 32)\n",
    "        \n",
    "        self.conv_last = nn.Sequential(nn.BatchNorm2d(32),\n",
    "                                     nn.MaxPool2d(2,2))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)   \n",
    "        \n",
    "        x = self.dconv_down4(x)\n",
    "        \n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        \n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv2], dim=1)       \n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)   \n",
    "        \n",
    "        x = self.dconv_up1(x)\n",
    "        \n",
    "        out = self.conv_last(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def attention_block():\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(1, 1, 1, padding=0),\n",
    "        nn.BatchNorm2d(1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels),\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True))\n",
    "\n",
    "\n",
    "def one_conv(in_channels, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels),\n",
    "        nn.Conv2d(in_channels, 1, 1, padding=padding))\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(1, 32)\n",
    "        self.dconv_down2 = double_conv(32, 64)\n",
    "        self.dconv_down3 = double_conv(64, 128)\n",
    "        self.dconv_down4 = double_conv(128, 256)        \n",
    "\n",
    "        self.maxpool     = nn.MaxPool2d(2)\n",
    "        self.upsample    = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
    "        self.oneconv     = one_conv\n",
    "        self.attention   = attention_block()\n",
    "        \n",
    "        self.oneconvx3 = one_conv(128)\n",
    "        self.oneconvg3 = one_conv(256)\n",
    "        self.dconv_up3 = double_conv(128 + 256, 128)\n",
    "        \n",
    "        self.oneconvx2 = one_conv(64)\n",
    "        self.oneconvg2 = one_conv(128)\n",
    "        self.dconv_up2 = double_conv(64 + 128, 64)\n",
    "        \n",
    "        \n",
    "        self.conv_last = nn.Sequential(nn.BatchNorm2d(64),\n",
    "                                     nn.Conv2d(64,32,3,padding=0),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.MaxPool2d(2,2),\n",
    "                                     nn.BatchNorm2d(32),\n",
    "                                     nn.Conv2d(32,8,3,padding=0),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.MaxPool2d(2,2))\n",
    "        \n",
    "        self.fc1 = nn.Sequential(nn.Linear(9800, 1096), \n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Dropout(0.8),\n",
    "                                     nn.Linear(1096, 96),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Dropout(0.9),\n",
    "                                     nn.Linear(96, 1))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        conv1 = self.dconv_down1(x) # 1 -> 32 filters\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x) # 32 -> 64 filters\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x) # 64 -> 128 filters\n",
    "        x = self.maxpool(conv3)   \n",
    "        \n",
    "        x = self.dconv_down4(x)     # 128 -> 256 filters\n",
    "        \n",
    "        x = self.upsample(x)        \n",
    "        _g = self.oneconvg3(x)\n",
    "        _x = self.oneconvx3(conv3)\n",
    "        _xg = _g + _x\n",
    "        psi = self.attention(_xg)\n",
    "        conv3 = conv3*psi\n",
    "        x = torch.cat([x, conv3], dim=1) \n",
    "        \n",
    "        x = self.dconv_up3(x)      # 128 + 256 -> 128 filters\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        _g = self.oneconvg2(x)\n",
    "        _x = self.oneconvx2(conv2)\n",
    "        _xg = _g + _x\n",
    "        psi = self.attention(_xg) \n",
    "        conv2 = conv2*psi\n",
    "        x = torch.cat([x, conv2], dim=1)       \n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        \n",
    "#         x = self.upsample(x)\n",
    "#         _g = self.oneconvg1(x)\n",
    "#         _x = self.oneconvx1(conv1)\n",
    "#         _xg = _g + _x\n",
    "#         psi = self.attention(_xg)\n",
    "#         conv1 = conv1*psi\n",
    "#         x = torch.cat([x, conv1], dim=1)   \n",
    "        \n",
    "#         x = self.dconv_up1(x)\n",
    "        \n",
    "        x = self.conv_last(x)\n",
    "        x = x.view(-1, 35*35*8)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm2d-1          [-1, 1, 296, 296]               2\n",
      "            Conv2d-2         [-1, 32, 296, 296]             320\n",
      "              ReLU-3         [-1, 32, 296, 296]               0\n",
      "       BatchNorm2d-4         [-1, 32, 296, 296]              64\n",
      "            Conv2d-5         [-1, 32, 296, 296]           9,248\n",
      "              ReLU-6         [-1, 32, 296, 296]               0\n",
      "         MaxPool2d-7         [-1, 32, 148, 148]               0\n",
      "       BatchNorm2d-8         [-1, 32, 148, 148]              64\n",
      "            Conv2d-9         [-1, 64, 148, 148]          18,496\n",
      "             ReLU-10         [-1, 64, 148, 148]               0\n",
      "      BatchNorm2d-11         [-1, 64, 148, 148]             128\n",
      "           Conv2d-12         [-1, 64, 148, 148]          36,928\n",
      "             ReLU-13         [-1, 64, 148, 148]               0\n",
      "        MaxPool2d-14           [-1, 64, 74, 74]               0\n",
      "      BatchNorm2d-15           [-1, 64, 74, 74]             128\n",
      "           Conv2d-16          [-1, 128, 74, 74]          73,856\n",
      "             ReLU-17          [-1, 128, 74, 74]               0\n",
      "      BatchNorm2d-18          [-1, 128, 74, 74]             256\n",
      "           Conv2d-19          [-1, 128, 74, 74]         147,584\n",
      "             ReLU-20          [-1, 128, 74, 74]               0\n",
      "        MaxPool2d-21          [-1, 128, 37, 37]               0\n",
      "      BatchNorm2d-22          [-1, 128, 37, 37]             256\n",
      "           Conv2d-23          [-1, 256, 37, 37]         295,168\n",
      "             ReLU-24          [-1, 256, 37, 37]               0\n",
      "      BatchNorm2d-25          [-1, 256, 37, 37]             512\n",
      "           Conv2d-26          [-1, 256, 37, 37]         590,080\n",
      "             ReLU-27          [-1, 256, 37, 37]               0\n",
      "         Upsample-28          [-1, 256, 74, 74]               0\n",
      "      BatchNorm2d-29          [-1, 256, 74, 74]             512\n",
      "           Conv2d-30            [-1, 1, 74, 74]             257\n",
      "      BatchNorm2d-31          [-1, 128, 74, 74]             256\n",
      "           Conv2d-32            [-1, 1, 74, 74]             129\n",
      "             ReLU-33            [-1, 1, 74, 74]               0\n",
      "           Conv2d-34            [-1, 1, 74, 74]               2\n",
      "      BatchNorm2d-35            [-1, 1, 74, 74]               2\n",
      "          Sigmoid-36            [-1, 1, 74, 74]               0\n",
      "      BatchNorm2d-37          [-1, 384, 74, 74]             768\n",
      "           Conv2d-38          [-1, 128, 74, 74]         442,496\n",
      "             ReLU-39          [-1, 128, 74, 74]               0\n",
      "      BatchNorm2d-40          [-1, 128, 74, 74]             256\n",
      "           Conv2d-41          [-1, 128, 74, 74]         147,584\n",
      "             ReLU-42          [-1, 128, 74, 74]               0\n",
      "         Upsample-43        [-1, 128, 148, 148]               0\n",
      "      BatchNorm2d-44        [-1, 128, 148, 148]             256\n",
      "           Conv2d-45          [-1, 1, 148, 148]             129\n",
      "      BatchNorm2d-46         [-1, 64, 148, 148]             128\n",
      "           Conv2d-47          [-1, 1, 148, 148]              65\n",
      "             ReLU-48          [-1, 1, 148, 148]               0\n",
      "           Conv2d-49          [-1, 1, 148, 148]               2\n",
      "      BatchNorm2d-50          [-1, 1, 148, 148]               2\n",
      "          Sigmoid-51          [-1, 1, 148, 148]               0\n",
      "      BatchNorm2d-52        [-1, 192, 148, 148]             384\n",
      "           Conv2d-53         [-1, 64, 148, 148]         110,656\n",
      "             ReLU-54         [-1, 64, 148, 148]               0\n",
      "      BatchNorm2d-55         [-1, 64, 148, 148]             128\n",
      "           Conv2d-56         [-1, 64, 148, 148]          36,928\n",
      "             ReLU-57         [-1, 64, 148, 148]               0\n",
      "      BatchNorm2d-58         [-1, 64, 148, 148]             128\n",
      "           Conv2d-59         [-1, 32, 146, 146]          18,464\n",
      "             ReLU-60         [-1, 32, 146, 146]               0\n",
      "        MaxPool2d-61           [-1, 32, 73, 73]               0\n",
      "      BatchNorm2d-62           [-1, 32, 73, 73]              64\n",
      "           Conv2d-63            [-1, 8, 71, 71]           2,312\n",
      "             ReLU-64            [-1, 8, 71, 71]               0\n",
      "        MaxPool2d-65            [-1, 8, 35, 35]               0\n",
      "           Linear-66                 [-1, 1096]      10,741,896\n",
      "             ReLU-67                 [-1, 1096]               0\n",
      "          Dropout-68                 [-1, 1096]               0\n",
      "           Linear-69                   [-1, 96]         105,312\n",
      "             ReLU-70                   [-1, 96]               0\n",
      "          Dropout-71                   [-1, 96]               0\n",
      "           Linear-72                    [-1, 1]              97\n",
      "================================================================\n",
      "Total params: 12,782,303\n",
      "Trainable params: 12,782,303\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.33\n",
      "Forward/backward pass size (MB): 454.16\n",
      "Params size (MB): 48.76\n",
      "Estimated Total Size (MB): 503.25\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "model = UNet(1).to(device)\n",
    "summary(model, (1, 296, 296))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trails (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.0976e+22, 1.8515e+28, 4.1988e+07],\n",
      "        [3.0357e+32, 2.7224e+20, 7.7782e+31],\n",
      "        [4.7429e+30, 1.3818e+31, 1.7225e+22],\n",
      "        [1.4602e-19, 1.8617e+25, 1.1835e+22],\n",
      "        [4.3066e+21, 6.3828e+28, 1.4603e-19]])\n",
      "tensor([[0.3337, 0.6211, 0.9639],\n",
      "        [0.1094, 0.2283, 0.4058],\n",
      "        [0.6591, 0.8595, 0.0782],\n",
      "        [0.7474, 0.8065, 0.0429],\n",
      "        [0.4577, 0.5123, 0.5054]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([[5.5000, 3.0000]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 1.6513, -0.3198, -1.5212],\n",
      "        [-1.4167, -0.5110, -1.1456],\n",
      "        [ 0.9274,  2.0594, -1.2510],\n",
      "        [ 0.0256, -0.2712, -0.4079],\n",
      "        [-0.0939, -1.1903,  1.3387]])\n"
     ]
    }
   ],
   "source": [
    "## TENSORS\n",
    "\n",
    "# create an 'un-initialized' matrix\n",
    "x = torch.empty(5, 3)\n",
    "print(x)\n",
    "\n",
    "# construct a randomly 'initialized' matrix\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "# construct a matrix filled with zeros an dtype=long\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)\n",
    "\n",
    "# construct a tensor from data\n",
    "x = torch.tensor([[5.5, 3]])\n",
    "print(x)\n",
    "\n",
    "# Create a tensor based on existing tensor\n",
    "x = x.new_ones(5, 3, dtype=torch.double)\n",
    "print(x)\n",
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.3659, -0.1678, -0.7175],\n",
      "        [-0.5564, -0.1421, -0.5350],\n",
      "        [ 1.0469,  3.0384, -0.9379],\n",
      "        [ 0.9468,  0.2249,  0.0415],\n",
      "        [ 0.0893, -0.8271,  1.6718]])\n",
      "tensor([[ 2.3659, -0.1678, -0.7175],\n",
      "        [-0.5564, -0.1421, -0.5350],\n",
      "        [ 1.0469,  3.0384, -0.9379],\n",
      "        [ 0.9468,  0.2249,  0.0415],\n",
      "        [ 0.0893, -0.8271,  1.6718]])\n",
      "tensor([[ 2.3659, -0.1678, -0.7175],\n",
      "        [-0.5564, -0.1421, -0.5350],\n",
      "        [ 1.0469,  3.0384, -0.9379],\n",
      "        [ 0.9468,  0.2249,  0.0415],\n",
      "        [ 0.0893, -0.8271,  1.6718]])\n",
      "tensor([[0.7146, 0.1521, 0.8037],\n",
      "        [0.8603, 0.3689, 0.6106],\n",
      "        [0.1195, 0.9790, 0.3132],\n",
      "        [0.9212, 0.4961, 0.4493],\n",
      "        [0.1832, 0.3632, 0.3331]])\n",
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "tensor([1.0785])\n",
      "1.0785350799560547\n"
     ]
    }
   ],
   "source": [
    "## OPERATIONS\n",
    "\n",
    "# Addition syntax 1\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)\n",
    "\n",
    "# Addition syntax 2\n",
    "print(torch.add(x, y))\n",
    "\n",
    "# Addtion output towards a tensor\n",
    "result = torch.empty(5,3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "\n",
    "# Addition in place\n",
    "y.add(x)\n",
    "print(y)\n",
    "\n",
    "# Any operation that mutates a tensor in-place is post-fixed with an _.\n",
    "x.copy_(y)\n",
    "x.t_()\n",
    "\n",
    "# Resizing tensors\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1,8)\n",
    "print(x.size(), y.size(), z.size())\n",
    "\n",
    "# Use get value off a one element tensor\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "## NUMPY BRIDGE\n",
    "\n",
    "# Torch tensor to numpy array\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "\n",
    "# Numpy array to torch tensor\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0785], device='cuda:0')\n",
      "tensor([2.0785], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "## USING CUDA\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")           # Cuda device object\n",
    "    y = torch.ones_like(x, device=device)   # Directly creates a tensor on GPU\n",
    "    x = x.to(device)                        # \n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAUTO-GRAD\\n- The autograd package provides automatic differntation for all\\nopeations on tensors. \\n- A define-by-run framework i.e backprop defined by how code \\nis run and every single iteration can be different.\\n\\nTENSOR\\n- torch.tensor is the central class of the 'torch' package.\\n- If  one sets attribute '.requires_grad()' as 'True', all \\noperations on it are tracked. \\n- When computations are finished one can call'backward()' \\nand have all the gradients computed.\\n- Gradient of a tensor is accumulated into '.grad' attribute.\\n- To stop tensor from tracking history, call '.detach()' to detach \\nit from computation history and prevent future computation \\nfrom being tracked\\n- To prevent tacking histroy and using memory, wrap the code \\nblock in 'with torch.no_grad()'. Helpful when evaluating a model\\ncause model has trainable parameters with 'requires_grad=True'\\n- 'Function' class is very important for autograd implementation\\n- 'Tensor' and 'Function' are interconnected and buid up an acyclic\\ngraph that encodes a complete history of computation.\\n- Each tensor has a '.grad_fn' attribute that references a 'Function'\\nthat has created the 'Tensor' (except for tensors created by user)\\n- To compute derivates, '.backward()' is called on a Tensor. If \\ntensor is a scalar, no arguments ought to be passed to '.backward()'\\nif not, a 'gradient' argument ought to be specified.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "AUTO-GRAD\n",
    "- The autograd package provides automatic differntation for all\n",
    "opeations on tensors. \n",
    "- A define-by-run framework i.e backprop defined by how code \n",
    "is run and every single iteration can be different.\n",
    "\n",
    "TENSOR\n",
    "- torch.tensor is the central class of the 'torch' package.\n",
    "- If  one sets attribute '.requires_grad()' as 'True', all \n",
    "operations on it are tracked. \n",
    "- When computations are finished one can call'backward()' \n",
    "and have all the gradients computed.\n",
    "- Gradient of a tensor is accumulated into '.grad' attribute.\n",
    "- To stop tensor from tracking history, call '.detach()' to detach \n",
    "it from computation history and prevent future computation \n",
    "from being tracked\n",
    "- To prevent tacking histroy and using memory, wrap the code \n",
    "block in 'with torch.no_grad()'. Helpful when evaluating a model\n",
    "cause model has trainable parameters with 'requires_grad=True'\n",
    "- 'Function' class is very important for autograd implementation\n",
    "- 'Tensor' and 'Function' are interconnected and buid up an acyclic\n",
    "graph that encodes a complete history of computation.\n",
    "- Each tensor has a '.grad_fn' attribute that references a 'Function'\n",
    "that has created the 'Tensor' (except for tensors created by user)\n",
    "- To compute derivates, '.backward()' is called on a Tensor. If \n",
    "tensor is a scalar, no arguments ought to be passed to '.backward()'\n",
    "if not, a 'gradient' argument ought to be specified.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "tensor([ -388.7856,   198.8780, -1300.0267], grad_fn=<MulBackward0>)\n",
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## TENSORS\n",
    "\n",
    "# Create tenor to track all operations\n",
    "x = torch.ones(2,2, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)\n",
    "\n",
    "## GRADIENTS\n",
    "\n",
    "# Peforming backprop on 'out'\n",
    "out.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# An example of vector-Jacobian product\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y)\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "print(x.grad)\n",
    "\n",
    "# Stop autograd from tracking history on Tensors \n",
    "# with .requires_grad=True \n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-847c9ce0eed7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not callable"
     ]
    }
   ],
   "source": [
    "image.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.5959, -1.3052, -0.6488,  ..., -1.0006, -1.8247,  1.6126],\n",
       "          [-1.0831,  1.6789, -0.2507,  ...,  1.9883,  0.0440, -1.0205],\n",
       "          [ 1.3978, -0.5599,  0.9209,  ...,  1.3029,  1.1875, -3.1398],\n",
       "          ...,\n",
       "          [-0.0280, -1.8147,  0.7449,  ..., -1.1217, -1.8393, -0.7728],\n",
       "          [-0.6970, -0.3968,  0.6772,  ..., -1.6072,  0.3949,  0.0676],\n",
       "          [-0.9794,  0.6049, -0.0923,  ...,  0.6333, -1.1131,  0.2632]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## NEURAL NETWORKS\\n\\n- Can be constructed using 'torch.nn' package\\n- 'nn' depends on 'autograd' to define models and differentiate\\nthem. \\n- 'nn.Module' contains layers and a method forward(input) that \\nreturns the 'output'.\\n- Training procedure:\\n    - Define neural network that has some learnable parameter\\n    - Iterate over a dataset of inputs\\n    - Process input through the network\\n    - Compute loss\\n    - Propagate gradients back into the network's parameters\\n    - Update weights\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "## NEURAL NETWORKS\n",
    "\n",
    "- Can be constructed using 'torch.nn' package\n",
    "- 'nn' depends on 'autograd' to define models and differentiate\n",
    "them. \n",
    "- 'nn.Module' contains layers and a method forward(input) that \n",
    "returns the 'output'.\n",
    "- Training procedure:\n",
    "    - Define neural network that has some learnable parameter\n",
    "    - Iterate over a dataset of inputs\n",
    "    - Process input through the network\n",
    "    - Compute loss\n",
    "    - Propagate gradients back into the network's parameters\n",
    "    - Update weights\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        # An affine operation \n",
    "        self.fc1 = nn.Linear(16*6*6, 128)\n",
    "        self.fc2 = nn.Linear(128, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        \n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "            \n",
    "net = Net()\n",
    "print(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
